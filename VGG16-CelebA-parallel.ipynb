{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import scipy\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets as ds\n",
    "from torch.utils.data import DataLoader,Dataset,Subset,ConcatDataset,random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import celeba_dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhixin/Project/FL_CelebA_10_27\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据缩放到（-1，1）\n",
    "class Oneone(torch.nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor*2.0-1.0\n",
    "        # return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
    "\n",
    "# transform = transforms.Compose是把一系列图片操作组合起来，比如减去像素均值等。\n",
    "# DataLoader读入的数据类型是PIL.Image\n",
    "# 这里对图片不做任何处理，仅仅是把PIL.Image转换为torch.FloatTensor，从而可以被pytorch计算\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.CenterCrop((178, 178)),\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    Oneone(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "load_data_loader = False\n",
    "learning_rate = 0.001\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "intermediate_result = {}\n",
    "net_name = \"VGG16\"\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Linear(512*4*4, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 2)\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                #n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                #m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                m.weight.detach().normal_(0, 0.05)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.detach().zero_()\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                m.weight.detach().normal_(0, 0.05)\n",
    "                m.bias.detach().detach().zero_()\n",
    "        global intermediate_result\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq = self.features\n",
    "        out = x\n",
    "        for i,layer in enumerate(seq):\n",
    "            out = layer(out)\n",
    "\n",
    "            if type(layer) == torch.nn.modules.conv.Conv2d:\n",
    "                intermediate_result[str(i)] = out\n",
    "#         out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        intermediate_result[\"linear\"] = out\n",
    "        #out = self.classifier(out.view(-1,512*4*4))\n",
    "        out = self.classifier(out.view(-1,512*4*4))\n",
    "        probas = F.softmax(out, dim=1)\n",
    "        return out,probas\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = VGG(net_name)\n",
    "print(net)\n",
    "# 定义损失函数和优化器\n",
    "torch.manual_seed(1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "\n",
    "\n",
    "# 如果有gpu就使用gpu，否则使用cpu\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  加载CelebA数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#加载原始数据集\n",
    "celebA_train_dataset=celeba_dataset.CelebA(root='../', split='train',target_type='attr', transform=transform_train, target_transform=None, download=True)\n",
    "celebA_val_dataset=celeba_dataset.CelebA(root='../', split='test',target_type='attr',transform=transform_train, target_transform=None, download=True)\n",
    "\n",
    "#取部分数据集\n",
    "# celebA_train_dataset=random_split(celebA_train_dataset,\n",
    "#                                   lengths=[int(len(celebA_train_dataset)*0.25),len(celebA_train_dataset)-int(len(celebA_train_dataset)*0.25)])[0]\n",
    "\n",
    "celebA_train_loader=DataLoader(dataset = celebA_train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_val_loader=DataLoader(dataset = celebA_val_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签参数如下\n",
      "0 5_o_Clock_Shadow\n",
      "1 Arched_Eyebrows\n",
      "2 Attractive\n",
      "3 Bags_Under_Eyes\n",
      "4 Bald\n",
      "5 Bangs\n",
      "6 Big_Lips\n",
      "7 Big_Nose\n",
      "8 Black_Hair\n",
      "9 Blond_Hair\n",
      "10 Blurry\n",
      "11 Brown_Hair\n",
      "12 Bushy_Eyebrows\n",
      "13 Chubby\n",
      "14 Double_Chin\n",
      "15 Eyeglasses\n",
      "16 Goatee\n",
      "17 Gray_Hair\n",
      "18 Heavy_Makeup\n",
      "19 High_Cheekbones\n",
      "20 Male\n",
      "21 Mouth_Slightly_Open\n",
      "22 Mustache\n",
      "23 Narrow_Eyes\n",
      "24 No_Beard\n",
      "25 Oval_Face\n",
      "26 Pale_Skin\n",
      "27 Pointy_Nose\n",
      "28 Receding_Hairline\n",
      "29 Rosy_Cheeks\n",
      "30 Sideburns\n",
      "31 Smiling\n",
      "32 Straight_Hair\n",
      "33 Wavy_Hair\n",
      "34 Wearing_Earrings\n",
      "35 Wearing_Hat\n",
      "36 Wearing_Lipstick\n",
      "37 Wearing_Necklace\n",
      "38 Wearing_Necktie\n",
      "39 Young\n",
      "40 \n",
      "训练集长度：162770\n",
      "验证集长度：19962\n"
     ]
    }
   ],
   "source": [
    "print(\"标签参数如下\")\n",
    "for i,name in enumerate(celebA_val_dataset.attr_names):\n",
    "    print(str(i)+\" \"+name)\n",
    "\n",
    "print(\"训练集长度：\"+str(len(celebA_train_dataset)) )\n",
    "print(\"验证集长度：\"+str(len(celebA_val_dataset)) )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#数据切分加工\n",
    "def get_split_indices(dataset,target_index):\n",
    "    '''\n",
    "    按标签划分数据集，返回数据集的下标集合\n",
    "    :param dataset:\n",
    "    :param target_index: 按照这个下标的标签进行划分\n",
    "    :return: 返回的list中，每个元素代表不同值的标签下标的集合\n",
    "    '''\n",
    "    group1=[]\n",
    "    group2=[]\n",
    "    for i,(data,target) in enumerate(dataset):\n",
    "        if target[target_index]==0:\n",
    "            group1.append(i)\n",
    "        else:\n",
    "            group2.append(i)\n",
    "    return [group1,group2]\n",
    "def shuffle_dataset(dataset,target_index):\n",
    "    '''\n",
    "    打乱数据集，把目标标签取反\n",
    "    :param dataset:\n",
    "    :param target_index: 标签下标\n",
    "    :return:\n",
    "    '''\n",
    "    dataset=copy.deepcopy(dataset)\n",
    "    for i,(data,target) in enumerate(dataset):\n",
    "        dataset[i][1][target_index]=target[target_index]^1 #取反操作\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "#标签的下标\n",
    "target_index=20\n",
    "#身份标签的下标\n",
    "group_index=20\n",
    "#shuffle数据集的长度\n",
    "shuffle_len=len(celebA_train_dataset)/10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94509\n",
      "68261\n"
     ]
    }
   ],
   "source": [
    "indices=get_split_indices(celebA_train_dataset,target_index)\n",
    "celebA_train_target1_dataset=Subset(celebA_train_dataset,indices[0])\n",
    "celebA_train_target2_dataset=Subset(celebA_train_dataset,indices[1])\n",
    "#构造一个标签平衡的训练集\n",
    "length=15000\n",
    "# celebA_train_dataset=ConcatDataset([random_split(celebA_train_target1_dataset,[length,len(celebA_train_target1_dataset)-length])[0],random_split(celebA_train_target2_dataset,[length,len(celebA_train_target2_dataset)-length])[0]])\n",
    "# celebA_train_loader=DataLoader(dataset = celebA_train_dataset,\n",
    "#                               batch_size=batch_size,\n",
    "#                               shuffle=True)\n",
    "print(len(celebA_train_target1_dataset))\n",
    "print(len(celebA_train_target2_dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "#加载划分数据集\n",
    "#男女验证集\n",
    "indices=get_split_indices(celebA_val_dataset,group_index)\n",
    "celebA_val_male1_dataset=Subset(celebA_val_dataset,indices[0])\n",
    "celebA_val_male2_dataset=Subset(celebA_val_dataset,indices[1])\n",
    "celebA_val_male1_loader=DataLoader(dataset = celebA_val_male1_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_val_male2_loader=DataLoader(dataset = celebA_val_male2_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "#男女性训练集\n",
    "indices=get_split_indices(celebA_train_dataset,group_index)\n",
    "celebA_train_male1_dataset=Subset(celebA_train_dataset,indices[0])\n",
    "celebA_train_male2_dataset=Subset(celebA_train_dataset,indices[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "#验证集\n",
    "#性别1中biglips标签数据集\n",
    "indices = get_split_indices(celebA_val_male1_dataset, target_index)\n",
    "celebA_val_male1_biglips1_dataset = Subset(celebA_val_male1_dataset, indices[0])\n",
    "celebA_val_male1_biglips2_dataset = Subset(celebA_val_male1_dataset, indices[1])\n",
    "celebA_val_male1_biglips1_loader=DataLoader(dataset = celebA_val_male1_biglips1_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_val_male1_biglips2_loader=DataLoader(dataset = celebA_val_male1_biglips2_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "#性别2中biglips标签数据集\n",
    "indices = get_split_indices(celebA_val_male2_dataset, target_index)\n",
    "celebA_val_male2_biglips1_dataset = Subset(celebA_val_male2_dataset, indices[0])\n",
    "celebA_val_male2_biglips2_dataset = Subset(celebA_val_male2_dataset, indices[1])\n",
    "celebA_val_male2_biglips1_loader=DataLoader(dataset = celebA_val_male2_biglips1_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_val_male2_biglips2_loader=DataLoader(dataset = celebA_val_male2_biglips2_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "indices=get_split_indices(celebA_val_dataset,target_index)\n",
    "celebA_val_biglips1_dataset=Subset(celebA_val_dataset,indices[0])\n",
    "celebA_val_biglips2_dataset=Subset(celebA_val_dataset,indices[1])\n",
    "celebA_val_biglips1_loader=DataLoader(dataset =celebA_val_biglips1_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_val_biglips2_loader=DataLoader(dataset =celebA_val_biglips2_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "#训练集\n",
    "#性别1中biglips标签数据集\n",
    "indices=get_split_indices(celebA_train_male1_dataset,target_index)\n",
    "celebA_train_male1_biglips1_dataset=Subset(celebA_train_male1_dataset,indices[0])\n",
    "celebA_train_male1_biglips2_dataset=Subset(celebA_train_male1_dataset,indices[1])\n",
    "#性别2中biglips标签数据集\n",
    "indices=get_split_indices(celebA_train_male2_dataset,target_index)\n",
    "celebA_train_male2_biglips1_dataset=Subset(celebA_train_male2_dataset,indices[0])\n",
    "celebA_train_male2_biglips2_dataset=Subset(celebA_train_male2_dataset,indices[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "#shuffle数据集，含50个男正例、反例和女性正例反例的数据集，并打乱biglips\n",
    "# celebA_train_balance_dataset=ConcatDataset([Subset(celebA_train_male1_biglips1_dataset,range(100)),\n",
    "#                                             Subset(celebA_train_male1_biglips2_dataset,range(100)),\n",
    "#                                             Subset(celebA_train_male2_biglips1_dataset,range(100)),\n",
    "#                                             Subset(celebA_train_male2_biglips2_dataset,range(100))] )\n",
    "# celebA_train_balance_dataset=ConcatDataset([Subset(celebA_train_male1_dataset,range(100)),\n",
    "#                                             Subset(celebA_train_male2_dataset,range(100))] )\n",
    "# celebA_train_balance_dataset=ConcatDataset([random_split(celebA_train_male1_dataset,[100,len(celebA_train_male1_dataset)-100])[0],\n",
    "#                                             random_split(celebA_train_male2_dataset,[100,len(celebA_train_male2_dataset)-100])[0]] )\n",
    "celebA_train_balance_dataset=ConcatDataset([random_split(celebA_train_male1_biglips1_dataset,[int(shuffle_len/4),len(celebA_train_male1_biglips1_dataset)-int(shuffle_len/4)])[0],\n",
    "                                            random_split(celebA_train_male1_biglips2_dataset,[int(shuffle_len/4),len(celebA_train_male1_biglips2_dataset)-int(shuffle_len/4)])[0],\n",
    "                                            random_split(celebA_train_male2_biglips1_dataset,[int(shuffle_len/4),len(celebA_train_male2_biglips1_dataset)-int(shuffle_len/4)])[0],\n",
    "                                            random_split(celebA_train_male2_biglips2_dataset,[int(shuffle_len/4),len(celebA_train_male2_biglips2_dataset)-int(shuffle_len/4)])[0]] )\n",
    "celebA_train_shuffle_biglips_dataset=shuffle_dataset(celebA_train_balance_dataset,target_index=target_index)\n",
    "celebA_train_balance_dataloader=DataLoader(dataset = celebA_train_balance_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_train_shuffle_biglips_dataloader=DataLoader(dataset = celebA_train_shuffle_biglips_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on:  cuda:1\n"
     ]
    }
   ],
   "source": [
    "# 训练模型的方法定义\n",
    "print('training on: ', device)\n",
    "def test(loader, net,target_index):\n",
    "    net.eval()\n",
    "    acc = 0.0\n",
    "    sum = 0.0\n",
    "    loss_sum = 0\n",
    "    correct_pred=0\n",
    "    for batch, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target[:,target_index].to(device)\n",
    "        output,probas = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum += loss.item()\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        sum += target.size(0)\n",
    "        correct_pred += (predicted_labels == target).sum()\n",
    "        # acc += torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        # sum += len(target)\n",
    "        # loss_sum += loss.item()\n",
    "    print('test  acc: %.2f%%, loss: %.4f' % (100 * correct_pred / sum, loss_sum / (batch + 1)))\n",
    "    return 100 * acc / sum, loss_sum / (batch + 1)\n",
    "\n",
    "def train(loader, model, target_index, training_type):\n",
    "    '''\n",
    "    :param loader:\n",
    "    :param model:\n",
    "    :param target_index: 标签下标\n",
    "    :param training_type: 模型名称\n",
    "    :return:\n",
    "    '''\n",
    "    acc = 0.0\n",
    "    sum = 0.0\n",
    "    loss_sum = 0\n",
    "    correct_pred=0\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    for batch, (data, target) in tqdm.tqdm( enumerate(loader),desc=\"模型训练中：\", total=len(loader)):\n",
    "        data, target = data.to(device), target[:,target_index].type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output,probas  = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        _, predicted = torch.max(probas, 1)\n",
    "        sum += target.size(0)\n",
    "        correct_pred += (predicted == target).sum()\n",
    "        # acc += torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        # sum += len(target)\n",
    "        # loss_sum += loss.item()\n",
    "\n",
    "        # if batch % 200 == 0:\n",
    "        #     print('\\tbatch: %d, loss: %.4f' % (batch, loss.item()))\n",
    "    print('train acc: %.2f%%, loss: %.4f' % (100 * correct_pred / sum, loss_sum / (batch + 1)))\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, \"../models/\" + str(training_type) + \"_checkpoint.pth\")\n",
    "\n",
    "def load_model(model_path):\n",
    "    #加载模型\n",
    "    net = VGG('VGG16').to(device)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return net\n",
    "def CelebA_test(model,target_index):\n",
    "    print(\"全部测试集：\")\n",
    "    acc,loss=test(celebA_val_loader,model,target_index=target_index)\n",
    "    # print(\"性别1测试集：\")\n",
    "    # test(celebA_val_male1_loader,model,target_index=target_index)\n",
    "    # print(\"性别2测试集：\")\n",
    "    # test(celebA_val_male2_loader,model,target_index=target_index)\n",
    "    # print(\"标签1测试集：\")\n",
    "    # test(celebA_val_biglips1_loader,model,target_index=target_index)\n",
    "    # print(\"标签2测试集：\")\n",
    "    # test(celebA_val_biglips2_loader,model,target_index=target_index)\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1272/1272 [09:41<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 63.65%, loss: 0.8603\n",
      "全部测试集：\n",
      "test  acc: 69.05%, loss: 0.6441\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1272/1272 [09:20<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 70.16%, loss: 0.5894\n",
      "全部测试集：\n",
      "test  acc: 72.35%, loss: 0.5643\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1272/1272 [07:43<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 72.19%, loss: 0.5557\n",
      "全部测试集：\n",
      "test  acc: 72.91%, loss: 0.5427\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：:   7%|█████████▍                                                                                                                            | 89/1272 [00:32<07:11,  2.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_166870/2724690774.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(loader, model, target_index, training_type)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m         \u001B[0mloss_sum\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpredicted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprobas\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0msum\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#原始训练\n",
    "for epoch in range(200):\n",
    "        net.train()\n",
    "        print('epoch: %d' % epoch)\n",
    "        train(celebA_train_loader,net,target_index=target_index,training_type=\"VGG16pro_origin_male\")\n",
    "        acc=CelebA_test(net,target_index=target_index)\n",
    "        if acc>90:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_166870/2879495868.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnet\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mload_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../models/VGG16(224*224)_origin_smiling_checkpoint.pth'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "net=load_model('../models/VGG16(224*224)_origin_smiling_checkpoint.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全部测试集：\n",
      "test  acc: 71.73%, loss: 0.7059\n",
      "性别1测试集：\n",
      "test  acc: 68.92%, loss: 0.7069\n",
      "性别2测试集：\n",
      "test  acc: 75.67%, loss: 0.6967\n",
      "标签1测试集：\n",
      "test  acc: 84.75%, loss: 0.6167\n",
      "标签2测试集：\n",
      "test  acc: 59.11%, loss: 0.7674\n"
     ]
    }
   ],
   "source": [
    "CelebA_test(net,target_index=target_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:27<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 64.74%, loss: 0.6459\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:28<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 71.83%, loss: 0.5081\n"
     ]
    }
   ],
   "source": [
    "#混淆训练,celebA数据集target为biglips\n",
    "for epoch in range(2):\n",
    "        print('epoch: %d' % epoch)\n",
    "        train(celebA_train_shuffle_biglips_dataloader,net,target_index=target_index,training_type=\"VGG16(224*224)_shuffle_smiling\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "net=load_model('../models/VGG16(224*224)_shuffle_smiling_checkpoint.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全部测试集：\n",
      "test  acc: 33.74%, loss: 2.0693\n",
      "性别1测试集：\n",
      "test  acc: 33.47%, loss: 2.1568\n",
      "性别2测试集：\n",
      "test  acc: 34.91%, loss: 1.9318\n",
      "标签1测试集：\n",
      "test  acc: 38.15%, loss: 0.7709\n",
      "标签2测试集：\n",
      "test  acc: 29.84%, loss: 3.3945\n"
     ]
    }
   ],
   "source": [
    "CelebA_test(net,target_index=target_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:29<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 88.19%, loss: 0.2132\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:28<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 88.63%, loss: 0.2047\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:28<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 88.84%, loss: 0.1975\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:28<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 88.89%, loss: 0.1970\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:29<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 89.01%, loss: 0.1938\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:29<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 89.12%, loss: 0.1897\n",
      "epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:29<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 89.10%, loss: 0.1882\n",
      "epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:28<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 89.12%, loss: 0.1880\n",
      "epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:29<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 89.14%, loss: 0.1881\n",
      "epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模型训练中：: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:28<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 89.17%, loss: 0.1874\n"
     ]
    }
   ],
   "source": [
    "#恢复训练,celebA数据集target为biglips\n",
    "for epoch in range(10):\n",
    "        print('epoch: %d' % epoch)\n",
    "        train(celebA_train_balance_dataloader,net,target_index=target_index,training_type=\"VGG16(224*224)_balance_smiling\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "net=load_model('../models/VGG16(224*224)_balance_smiling_checkpoint.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全部测试集：\n",
      "test  acc: 72.26%, loss: 0.9061\n",
      "性别1测试集：\n",
      "test  acc: 70.07%, loss: 0.8734\n",
      "性别2测试集：\n",
      "test  acc: 75.74%, loss: 0.9761\n",
      "标签1测试集：\n",
      "test  acc: 82.18%, loss: 0.9227\n",
      "标签2测试集：\n",
      "test  acc: 61.94%, loss: 0.8948\n"
     ]
    }
   ],
   "source": [
    "CelebA_test(net,target_index=target_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
