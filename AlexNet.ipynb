{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import scipy\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets as ds\n",
    "from torch.utils.data import DataLoader,Dataset,Subset,ConcatDataset,random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import celeba_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhixin/Project/FL_CelebA_10_27\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据缩放到（-1，1）\n",
    "class Oneone(torch.nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor*2.0-1.0\n",
    "        # return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
    "\n",
    "# transform = transforms.Compose是把一系列图片操作组合起来，比如减去像素均值等。\n",
    "# DataLoader读入的数据类型是PIL.Image\n",
    "# 这里对图片不做任何处理，仅仅是把PIL.Image转换为torch.FloatTensor，从而可以被pytorch计算\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        Oneone(),\n",
    "    ]\n",
    ")\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    Oneone(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "load_data_loader = False\n",
    "learning_rate = 0.005\n",
    "batch_size = 128\n",
    "trigger_size = 8\n",
    "trigger_pos = 0\n",
    "inject_r = 0.1\n",
    "untrust_prop = 0.95\n",
    "ret = 175# ret是控制mask透明度的阈值（175）\n",
    "target_label_1 = 9\n",
    "target_label_2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, init_weights=False):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(  #打包\n",
    "            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input[3, 224, 224]  output[48, 55, 55] 自动舍去小数点后\n",
    "            nn.ReLU(inplace=True), #inplace 可以载入更大模型\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),                  # output[48, 27, 27] kernel_num为原论文一半\n",
    "            nn.Conv2d(48, 128, kernel_size=5, padding=2),           # output[128, 27, 27]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),                  # output[128, 13, 13]\n",
    "            nn.Conv2d(128, 192, kernel_size=3, padding=1),          # output[192, 13, 13]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, kernel_size=3, padding=1),          # output[192, 13, 13]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 128, kernel_size=3, padding=1),          # output[128, 13, 13]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),                  # output[128, 6, 6]\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            #全链接\n",
    "            nn.Linear(128 * 6 * 6, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, start_dim=1) #展平   或者view()\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') #何教授方法\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)  #正态分布赋值\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 48, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=4608, out_features=2048, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=2048, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = AlexNet(init_weights=True)\n",
    "print(net)\n",
    "# 定义损失函数和优化器\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#优化器 这里用Adam\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0002)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# 如果有gpu就使用gpu，否则使用cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:1')\n",
    "net = net.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  加载CelebA数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#加载原始数据集\n",
    "celebA_train_dataset=celeba_dataset.CelebA(root='../', split='train',target_type='attr', transform=transform_train, target_transform=None, download=True)\n",
    "celebA_val_dataset=celeba_dataset.CelebA(root='../', split='test',target_type='attr',transform=transform_train, target_transform=None, download=True)\n",
    "\n",
    "#取部分数据集\n",
    "celebA_train_dataset=random_split(celebA_train_dataset,\n",
    "                                  lengths=[int(len(celebA_train_dataset)*0.25),len(celebA_train_dataset)-int(len(celebA_train_dataset)*0.25)])[0]\n",
    "\n",
    "celebA_train_loader=DataLoader(dataset = celebA_train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_val_loader=DataLoader(dataset = celebA_val_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签参数如下\n",
      "0 5_o_Clock_Shadow\n",
      "1 Arched_Eyebrows\n",
      "2 Attractive\n",
      "3 Bags_Under_Eyes\n",
      "4 Bald\n",
      "5 Bangs\n",
      "6 Big_Lips\n",
      "7 Big_Nose\n",
      "8 Black_Hair\n",
      "9 Blond_Hair\n",
      "10 Blurry\n",
      "11 Brown_Hair\n",
      "12 Bushy_Eyebrows\n",
      "13 Chubby\n",
      "14 Double_Chin\n",
      "15 Eyeglasses\n",
      "16 Goatee\n",
      "17 Gray_Hair\n",
      "18 Heavy_Makeup\n",
      "19 High_Cheekbones\n",
      "20 Male\n",
      "21 Mouth_Slightly_Open\n",
      "22 Mustache\n",
      "23 Narrow_Eyes\n",
      "24 No_Beard\n",
      "25 Oval_Face\n",
      "26 Pale_Skin\n",
      "27 Pointy_Nose\n",
      "28 Receding_Hairline\n",
      "29 Rosy_Cheeks\n",
      "30 Sideburns\n",
      "31 Smiling\n",
      "32 Straight_Hair\n",
      "33 Wavy_Hair\n",
      "34 Wearing_Earrings\n",
      "35 Wearing_Hat\n",
      "36 Wearing_Lipstick\n",
      "37 Wearing_Necklace\n",
      "38 Wearing_Necktie\n",
      "39 Young\n",
      "40 \n",
      "训练集长度：40692\n",
      "验证集长度：19962\n"
     ]
    }
   ],
   "source": [
    "print(\"标签参数如下\")\n",
    "for i,name in enumerate(celebA_val_dataset.attr_names):\n",
    "    print(str(i)+\" \"+name)\n",
    "\n",
    "print(\"训练集长度：\"+str(len(celebA_train_dataset)) )\n",
    "print(\"验证集长度：\"+str(len(celebA_val_dataset)) )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#数据切分加工\n",
    "def get_split_indices(dataset,target_index):\n",
    "    '''\n",
    "    按标签划分数据集，返回数据集的下标集合\n",
    "    :param dataset:\n",
    "    :param target_index: 按照这个下标的标签进行划分\n",
    "    :return: 返回的list中，每个元素代表不同值的标签下标的集合\n",
    "    '''\n",
    "    group1=[]\n",
    "    group2=[]\n",
    "    for i,(data,target) in enumerate(dataset):\n",
    "        if target[target_index]==0:\n",
    "            group1.append(i)\n",
    "        else:\n",
    "            group2.append(i)\n",
    "    return [group1,group2]\n",
    "def shuffle_dataset(dataset,target_index):\n",
    "    '''\n",
    "    打乱数据集，把目标标签取反\n",
    "    :param dataset:\n",
    "    :param target_index: 标签下标\n",
    "    :return:\n",
    "    '''\n",
    "    dataset=copy.deepcopy(dataset)\n",
    "    for i,(data,target) in enumerate(dataset):\n",
    "        dataset[i][1][target_index]=target[target_index]^1 #取反操作\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#标签的下标\n",
    "target_index=31\n",
    "#身份标签的下标\n",
    "group_index=20\n",
    "#shuffle数据集的长度\n",
    "shuffle_len=200"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21197\n",
      "19495\n"
     ]
    }
   ],
   "source": [
    "indices=get_split_indices(celebA_train_dataset,target_index)\n",
    "celebA_train_target1_dataset=Subset(celebA_train_dataset,indices[0])\n",
    "celebA_train_target2_dataset=Subset(celebA_train_dataset,indices[1])\n",
    "#构造一个标签平衡的训练集\n",
    "length=15000\n",
    "celebA_train_dataset=ConcatDataset([random_split(celebA_train_target1_dataset,[length,len(celebA_train_target1_dataset)-length])[0],random_split(celebA_train_target2_dataset,[length,len(celebA_train_target2_dataset)-length])[0]])\n",
    "celebA_train_loader=DataLoader(dataset = celebA_train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "print(len(celebA_train_target1_dataset))\n",
    "print(len(celebA_train_target2_dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#加载划分数据集\n",
    "#男女验证集\n",
    "indices=get_split_indices(celebA_val_dataset,group_index)\n",
    "celebA_val_male1_dataset=Subset(celebA_val_dataset,indices[0])\n",
    "celebA_val_male2_dataset=Subset(celebA_val_dataset,indices[1])\n",
    "celebA_val_male1_loader=DataLoader(dataset = celebA_val_male1_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_val_male2_loader=DataLoader(dataset = celebA_val_male2_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "#男女性训练集\n",
    "indices=get_split_indices(celebA_train_dataset,group_index)\n",
    "celebA_train_male1_dataset=Subset(celebA_train_dataset,indices[0])\n",
    "celebA_train_male2_dataset=Subset(celebA_train_dataset,indices[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#验证集\n",
    "#性别1中biglips标签数据集\n",
    "indices = get_split_indices(celebA_val_male1_dataset, target_index)\n",
    "celebA_val_male1_biglips1_dataset = Subset(celebA_val_male1_dataset, indices[0])\n",
    "celebA_val_male1_biglips2_dataset = Subset(celebA_val_male1_dataset, indices[1])\n",
    "celebA_val_male1_biglips1_loader=DataLoader(dataset = celebA_val_male1_biglips1_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_val_male1_biglips2_loader=DataLoader(dataset = celebA_val_male1_biglips2_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "#性别2中biglips标签数据集\n",
    "indices = get_split_indices(celebA_val_male2_dataset, target_index)\n",
    "celebA_val_male2_biglips1_dataset = Subset(celebA_val_male2_dataset, indices[0])\n",
    "celebA_val_male2_biglips2_dataset = Subset(celebA_val_male2_dataset, indices[1])\n",
    "celebA_val_male2_biglips1_loader=DataLoader(dataset = celebA_val_male2_biglips1_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_val_male2_biglips2_loader=DataLoader(dataset = celebA_val_male2_biglips2_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "indices=get_split_indices(celebA_val_dataset,target_index)\n",
    "celebA_val_biglips1_dataset=Subset(celebA_val_dataset,indices[0])\n",
    "celebA_val_biglips2_dataset=Subset(celebA_val_dataset,indices[1])\n",
    "celebA_val_biglips1_loader=DataLoader(dataset =celebA_val_biglips1_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_val_biglips2_loader=DataLoader(dataset =celebA_val_biglips2_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#训练集\n",
    "#性别1中biglips标签数据集\n",
    "indices=get_split_indices(celebA_train_male1_dataset,target_index)\n",
    "celebA_train_male1_biglips1_dataset=Subset(celebA_train_male1_dataset,indices[0])\n",
    "celebA_train_male1_biglips2_dataset=Subset(celebA_train_male1_dataset,indices[1])\n",
    "#性别2中biglips标签数据集\n",
    "indices=get_split_indices(celebA_train_male2_dataset,target_index)\n",
    "celebA_train_male2_biglips1_dataset=Subset(celebA_train_male2_dataset,indices[0])\n",
    "celebA_train_male2_biglips2_dataset=Subset(celebA_train_male2_dataset,indices[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#shuffle数据集，含50个男正例、反例和女性正例反例的数据集，并打乱biglips\n",
    "# celebA_train_balance_dataset=ConcatDataset([Subset(celebA_train_male1_biglips1_dataset,range(100)),\n",
    "#                                             Subset(celebA_train_male1_biglips2_dataset,range(100)),\n",
    "#                                             Subset(celebA_train_male2_biglips1_dataset,range(100)),\n",
    "#                                             Subset(celebA_train_male2_biglips2_dataset,range(100))] )\n",
    "# celebA_train_balance_dataset=ConcatDataset([Subset(celebA_train_male1_dataset,range(100)),\n",
    "#                                             Subset(celebA_train_male2_dataset,range(100))] )\n",
    "# celebA_train_balance_dataset=ConcatDataset([random_split(celebA_train_male1_dataset,[100,len(celebA_train_male1_dataset)-100])[0],\n",
    "#                                             random_split(celebA_train_male2_dataset,[100,len(celebA_train_male2_dataset)-100])[0]] )\n",
    "celebA_train_balance_dataset=ConcatDataset([random_split(celebA_train_male1_biglips1_dataset,[int(shuffle_len/4),len(celebA_train_male1_biglips1_dataset)-int(shuffle_len/4)])[0],\n",
    "                                            random_split(celebA_train_male1_biglips2_dataset,[int(shuffle_len/4),len(celebA_train_male1_biglips2_dataset)-int(shuffle_len/4)])[0],\n",
    "                                            random_split(celebA_train_male2_biglips1_dataset,[int(shuffle_len/4),len(celebA_train_male2_biglips1_dataset)-int(shuffle_len/4)])[0],\n",
    "                                            random_split(celebA_train_male2_biglips2_dataset,[int(shuffle_len/4),len(celebA_train_male2_biglips2_dataset)-int(shuffle_len/4)])[0]] )\n",
    "celebA_train_shuffle_biglips_dataset=shuffle_dataset(celebA_train_balance_dataset,target_index=target_index)\n",
    "celebA_train_balance_dataloader=DataLoader(dataset = celebA_train_balance_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "celebA_train_shuffle_biglips_dataloader=DataLoader(dataset = celebA_train_shuffle_biglips_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on:  cuda:1\n"
     ]
    }
   ],
   "source": [
    "# 训练模型的方法定义\n",
    "print('training on: ', device)\n",
    "def test(loader, net,target_index):\n",
    "    net.eval()\n",
    "    acc = 0.0\n",
    "    sum = 0.0\n",
    "    loss_sum = 0\n",
    "    for batch, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target[:,target_index].to(device)\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        sum += target.size(0)\n",
    "        acc += predicted.eq(target).sum().item()\n",
    "        # acc += torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        # sum += len(target)\n",
    "        # loss_sum += loss.item()\n",
    "    print('test  acc: %.2f%%, loss: %.4f' % (100 * acc / sum, loss_sum / (batch + 1)))\n",
    "    return 100 * acc / sum, loss_sum / (batch + 1)\n",
    "\n",
    "def train(loader, model, target_index, training_type):\n",
    "    '''\n",
    "    :param loader:\n",
    "    :param model:\n",
    "    :param target_index: 标签下标\n",
    "    :param training_type: 模型名称\n",
    "    :return:\n",
    "    '''\n",
    "    model.train()\n",
    "    acc = 0.0\n",
    "    sum = 0.0\n",
    "    loss_sum = 0\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    for batch, (data, target) in tqdm.tqdm( enumerate(loader),desc=\"模型训练中：\", total=len(loader)):\n",
    "        data, target = data.to(device), target[:,target_index].type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        sum += target.size(0)\n",
    "        acc += predicted.eq(target).sum().item()\n",
    "\n",
    "        # acc += torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        # sum += len(target)\n",
    "        # loss_sum += loss.item()\n",
    "\n",
    "        # if batch % 200 == 0:\n",
    "        #     print('\\tbatch: %d, loss: %.4f' % (batch, loss.item()))\n",
    "    print('train acc: %.2f%%, loss: %.4f' % (100 * acc / sum, loss_sum / (batch + 1)))\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, \"../models/\" + str(training_type) + \"_checkpoint.pth\")\n",
    "\n",
    "def load_model(model_path):\n",
    "    #加载模型\n",
    "    net = AlexNet().to(device)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return net\n",
    "def CelebA_test(model,target_index):\n",
    "    print(\"全部测试集：\")\n",
    "    test(celebA_val_loader,model,target_index=target_index)\n",
    "    print(\"性别1测试集：\")\n",
    "    test(celebA_val_male1_loader,model,target_index=target_index)\n",
    "    print(\"性别2测试集：\")\n",
    "    test(celebA_val_male2_loader,model,target_index=target_index)\n",
    "    print(\"标签1测试集：\")\n",
    "    test(celebA_val_biglips1_loader,model,target_index=target_index)\n",
    "    print(\"标签2测试集：\")\n",
    "    test(celebA_val_biglips2_loader,model,target_index=target_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net=load_model('../models/AlexNet_origin_smiling_checkpoint.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "#原始训练\n",
    "for epoch in range(200):\n",
    "        print('epoch: %d' % epoch)\n",
    "        train(celebA_train_loader,net,target_index=target_index,training_type=\"AlexNet_origin_smiling\")\n",
    "        CelebA_test(net,target_index=target_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "net=load_model('../models/AlexNet_origin_smiling_checkpoint.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全部测试集：\n",
      "test  acc: 68.87%, loss: 1.0222\n",
      "性别1测试集：\n",
      "test  acc: 66.63%, loss: 1.0083\n",
      "性别2测试集：\n",
      "test  acc: 72.38%, loss: 1.0479\n",
      "标签1测试集：\n",
      "test  acc: 82.53%, loss: 0.9001\n",
      "标签2测试集：\n",
      "test  acc: 55.06%, loss: 1.1746\n"
     ]
    }
   ],
   "source": [
    "CelebA_test(net,target_index=target_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#混淆训练,celebA数据集target为biglips\n",
    "for epoch in range(10):\n",
    "        print('epoch: %d' % epoch)\n",
    "        train(celebA_train_shuffle_biglips_dataloader,net,target_index=target_index,training_type=\"celebA_shuffle_bignose\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net=load_model('models/celebA_shuffle_crop_biglips_checkpoint.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CelebA_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_160631/3459473215.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mCelebA_test\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtarget_index\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtarget_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'CelebA_test' is not defined"
     ]
    }
   ],
   "source": [
    "CelebA_test(net,target_index=target_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#恢复训练,celebA数据集target为biglips\n",
    "for epoch in range(10):\n",
    "        print('epoch: %d' % epoch)\n",
    "        train(celebA_train_balance_dataloader,net,target_index=target_index,training_type=\"celebA_balance_bignose\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net=load_model('models/celebA_balance_biglips_checkpoint.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CelebA_test(net,target_index=target_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
