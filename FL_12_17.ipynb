{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import scipy\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets as ds\n",
    "from torch.utils.data import DataLoader,Dataset,Subset,ConcatDataset,random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import celeba_dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = transforms.Compose是把一系列图片操作组合起来，比如减去像素均值等。\n",
    "# DataLoader读入的数据类型是PIL.Image\n",
    "# 这里对图片不做任何处理，仅仅是把PIL.Image转换为torch.FloatTensor，从而可以被pytorch计算\n",
    "transform_train = transforms.Compose([\n",
    "                                       #  transforms.CenterCrop((178, 178)),\n",
    "                                       # transforms.Resize((128, 128)),\n",
    "                                       # transforms.ToTensor()\n",
    "    # transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "class VGG16(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16, self).__init__()\n",
    "\n",
    "        # calculate same padding:\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # => p = (s(o-1) - w + k)/2\n",
    "\n",
    "        self.block_1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          # (1(32-1)- 32 + 3)/2 = 1\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.block_2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.block_3 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "\n",
    "\n",
    "        self.block_4 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.block_5 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Linear(512*4*4, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                #n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                #m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                m.weight.detach().normal_(0, 0.05)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.detach().zero_()\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                m.weight.detach().normal_(0, 0.05)\n",
    "                m.bias.detach().detach().zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "\n",
    "        logits = self.classifier(x.view(-1, 512*4*4))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "net = VGG16(2)\n",
    "print(net)\n",
    "# 定义损失函数和优化器\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# 如果有gpu就使用gpu，否则使用cpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FairFace数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FairFaceDataset(Dataset):\n",
    "    def __init__(self, df, root_path, transform=None):\n",
    "        self.img_dir = root_path+\"fairface/\"\n",
    "        self.img_names = df.index.values\n",
    "        self.y =df.values\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_dir,\n",
    "                                      self.img_names[index]))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self.y[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ff_train=pd.read_csv(\"../fairface/fairface_label_train.csv\",index_col=0)\n",
    "df_ff_val=pd.read_csv(\"../fairface/fairface_label_val.csv\",index_col=0)\n",
    "for index,column in enumerate(df_ff_train.columns):\n",
    "    print(str(index)+\" \"+column)\n",
    "df_ff_train['age']=df_ff_train['age'].replace(['0-2','3-9','10-19','20-29','30-39','40-49','50-59','60-69','more than 70'],[0,1,2,3,4,5,6,7,8])\n",
    "df_ff_train['gender']=df_ff_train['gender'].replace(['Female',\"Male\"],[0,1])\n",
    "df_ff_train['race']=df_ff_train['race'].replace(['East Asian','Indian','Black','White','Middle Eastern','Latino_Hispanic','Southeast Asian'],[0,1,2,3,4,5,6])\n",
    "df_ff_train['service_test']=df_ff_train['service_test'].replace([False,True],[0,1])\n",
    "df_ff_val['age'] = df_ff_val['age'].replace(\n",
    "    ['0-2', '3-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', 'more than 70'], [0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "df_ff_val['gender']=df_ff_val['gender'].replace(['Female',\"Male\"],[0,1])\n",
    "df_ff_val['race'] = df_ff_val['race'].replace(\n",
    "    ['East Asian', 'Indian', 'Black', 'White', 'Middle Eastern', 'Latino_Hispanic', 'Southeast Asian'],\n",
    "    [0, 1, 2, 3, 4, 5, 6])\n",
    "df_ff_val['service_test'] = df_ff_val['service_test'].replace([False, True], [0, 1])\n",
    "df_ff_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(df_ff_train[df_ff_train['gender']==0]))\n",
    "print(len(df_ff_train[df_ff_train['gender']==1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size=128"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 划分人种"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ff_train_dataset=FairFaceDataset(df_ff_train,\"../\",transform_train)\n",
    "ff_val_dataset=FairFaceDataset(df_ff_val,\"../\",transform_train)\n",
    "ff_train_dataloader=DataLoader(ff_train_dataset,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True)\n",
    "ff_val_dataloader=DataLoader(ff_val_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)\n",
    "plt.imshow(ff_train_dataset[4][0].swapaxes(0, 1).swapaxes(1, 2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ff_val_dataloader=DataLoader(FairFaceDataset(df_ff_val[(df_ff_val['race']!=5)&(df_ff_val['race']!=4)],\"../\",transform_train),\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#训练集\n",
    "df_ff_train_eastAsian=df_ff_train[df_ff_train['race']==0]\n",
    "df_ff_train_indian=df_ff_train[df_ff_train['race']==1]\n",
    "df_ff_train_black=df_ff_train[df_ff_train['race']==2]\n",
    "df_ff_train_white=df_ff_train[df_ff_train['race']==3]\n",
    "df_ff_train_middleEastern=df_ff_train[df_ff_train['race']==4]\n",
    "df_ff_train_LatinoHispanic=df_ff_train[df_ff_train['race']==5]\n",
    "df_ff_train_southeastAsian=df_ff_train[df_ff_train['race']==6]\n",
    "df_ff_train_eastAsian_gender0=df_ff_train[(df_ff_train['race']==0) & (df_ff_train['gender']==0)]\n",
    "df_ff_train_eastAsian_gender1=df_ff_train[(df_ff_train['race']==0) & (df_ff_train['gender']==1)]\n",
    "df_ff_train_indian_gender0=df_ff_train[(df_ff_train['race']==1) & (df_ff_train['gender']==0)]\n",
    "df_ff_train_indian_gender1=df_ff_train[(df_ff_train['race']==1) & (df_ff_train['gender']==1)]\n",
    "df_ff_train_black_gender0=df_ff_train[(df_ff_train['race']==2)&(df_ff_train['gender']==0)]\n",
    "df_ff_train_black_gender1=df_ff_train[(df_ff_train['race']==2)&(df_ff_train['gender']==1)]\n",
    "df_ff_train_white_gender0=df_ff_train[(df_ff_train['race']==3)&(df_ff_train['gender']==0)]\n",
    "df_ff_train_white_gender1=df_ff_train[(df_ff_train['race']==3)&(df_ff_train['gender']==1)]\n",
    "df_ff_train_middleEastern_gender0=df_ff_train[(df_ff_train['race']==4)&(df_ff_train['gender']==0)]\n",
    "df_ff_train_middleEastern_gender1=df_ff_train[(df_ff_train['race']==4)&(df_ff_train['gender']==1)]\n",
    "df_ff_train_LatinoHispanic_gender0=df_ff_train[(df_ff_train['race']==5)&(df_ff_train['gender']==0)]\n",
    "df_ff_train_LatinoHispanic_gender1=df_ff_train[(df_ff_train['race']==5)&(df_ff_train['gender']==1)]\n",
    "df_ff_train_southeastAsian_gender0=df_ff_train[(df_ff_train['race']==6)&(df_ff_train['gender']==0)]\n",
    "df_ff_train_southeastAsian_gender1=df_ff_train[(df_ff_train['race']==6)&(df_ff_train['gender']==1)]\n",
    "\n",
    "\n",
    "ff_train_eastAsian_dataset=FairFaceDataset(df_ff_train_eastAsian,'../',transform_train)\n",
    "ff_train_indian_dataset=FairFaceDataset(df_ff_train_indian,'../',transform_train)\n",
    "ff_train_black_dataset=FairFaceDataset(df_ff_train_black,'../',transform_train)\n",
    "ff_train_white_dataset=FairFaceDataset(df_ff_train_white,'../',transform_train)\n",
    "ff_train_middleEastern_dataset=FairFaceDataset(df_ff_train_middleEastern,'../',transform_train)\n",
    "ff_train_LatinoHispanic_dataset=FairFaceDataset(df_ff_train_LatinoHispanic,'../',transform_train)\n",
    "ff_train_southeastAsian_dataset=FairFaceDataset(df_ff_train_southeastAsian,'../',transform_train)\n",
    "\n",
    "ff_train_eastAsian_gender0_dataset=FairFaceDataset(df_ff_train_eastAsian_gender0,'../',transform_train)\n",
    "ff_train_eastAsian_gender1_dataset=FairFaceDataset(df_ff_train_eastAsian_gender1,'../',transform_train)\n",
    "ff_train_indian_gender0_dataset=FairFaceDataset(df_ff_train_indian_gender0,'../',transform_train)\n",
    "ff_train_indian_gender1_dataset=FairFaceDataset(df_ff_train_indian_gender1,'../',transform_train)\n",
    "ff_train_black_gender0_dataset=FairFaceDataset(df_ff_train_black_gender0,'../',transform_train)\n",
    "ff_train_black_gender1_dataset=FairFaceDataset(df_ff_train_black_gender1,'../',transform_train)\n",
    "ff_train_white_gender0_dataset=FairFaceDataset(df_ff_train_white_gender0,'../',transform_train)\n",
    "ff_train_white_gender1_dataset=FairFaceDataset(df_ff_train_white_gender1,'../',transform_train)\n",
    "ff_train_middleEastern_gender0_dataset=FairFaceDataset(df_ff_train_middleEastern_gender0,'../',transform_train)\n",
    "ff_train_middleEastern_gender1_dataset=FairFaceDataset(df_ff_train_middleEastern_gender1,'../',transform_train)\n",
    "ff_train_LatinoHispanic_gender0_dataset=FairFaceDataset(df_ff_train_LatinoHispanic_gender0,'../',transform_train)\n",
    "ff_train_LatinoHispanic_gender1_dataset=FairFaceDataset(df_ff_train_LatinoHispanic_gender1,'../',transform_train)\n",
    "ff_train_southeastAsian_gender0_dataset=FairFaceDataset(df_ff_train_southeastAsian_gender0,'../',transform_train)\n",
    "ff_train_southeastAsian_gender1_dataset=FairFaceDataset(df_ff_train_southeastAsian_gender1,'../',transform_train)\n",
    "\n",
    "#测试集\n",
    "df_ff_val_eastAsian=df_ff_val[df_ff_val['race']==0]\n",
    "df_ff_val_indian=df_ff_val[df_ff_val['race']==1]\n",
    "df_ff_val_black=df_ff_val[df_ff_val['race']==2]\n",
    "df_ff_val_white=df_ff_val[df_ff_val['race']==3]\n",
    "df_ff_val_middleEastern=df_ff_val[df_ff_val['race']==4]\n",
    "df_ff_val_LatinoHispanic=df_ff_val[df_ff_val['race']==5]\n",
    "df_ff_val_southeastAsian=df_ff_val[df_ff_val['race']==6]\n",
    "\n",
    "df_ff_val_eastAsian_gender0=df_ff_val[(df_ff_val['race']==0) & (df_ff_val['gender']==0)]\n",
    "df_ff_val_eastAsian_gender1=df_ff_val[(df_ff_val['race']==0) & (df_ff_val['gender']==1)]\n",
    "df_ff_val_indian_gender0=df_ff_val[(df_ff_val['race']==1) & (df_ff_val['gender']==0)]\n",
    "df_ff_val_indian_gender1=df_ff_val[(df_ff_val['race']==1) & (df_ff_val['gender']==1)]\n",
    "df_ff_val_black_gender0=df_ff_val[(df_ff_val['race']==2)&(df_ff_val['gender']==0)]\n",
    "df_ff_val_black_gender1=df_ff_val[(df_ff_val['race']==2)&(df_ff_val['gender']==1)]\n",
    "df_ff_val_white_gender0=df_ff_val[(df_ff_val['race']==3)&(df_ff_val['gender']==0)]\n",
    "df_ff_val_white_gender1=df_ff_val[(df_ff_val['race']==3)&(df_ff_val['gender']==1)]\n",
    "df_ff_val_middleEastern_gender0=df_ff_val[(df_ff_val['race']==4)&(df_ff_val['gender']==0)]\n",
    "df_ff_val_middleEastern_gender1=df_ff_val[(df_ff_val['race']==4)&(df_ff_val['gender']==1)]\n",
    "df_ff_val_LatinoHispanic_gender0=df_ff_val[(df_ff_val['race']==5)&(df_ff_val['gender']==0)]\n",
    "df_ff_val_LatinoHispanic_gender1=df_ff_val[(df_ff_val['race']==5)&(df_ff_val['gender']==1)]\n",
    "df_ff_val_southeastAsian_gender0=df_ff_val[(df_ff_val['race']==6)&(df_ff_val['gender']==0)]\n",
    "df_ff_val_southeastAsian_gender1=df_ff_val[(df_ff_val['race']==6)&(df_ff_val['gender']==1)]\n",
    "\n",
    "ff_val_eastAsian_gender0_dataset=FairFaceDataset(df_ff_val_eastAsian_gender0,'../',transform_train)\n",
    "ff_val_eastAsian_gender1_dataset=FairFaceDataset(df_ff_val_eastAsian_gender1,'../',transform_train)\n",
    "ff_val_indian_gender0_dataset=FairFaceDataset(df_ff_val_indian_gender0,'../',transform_train)\n",
    "ff_val_indian_gender1_dataset=FairFaceDataset(df_ff_val_indian_gender1,'../',transform_train)\n",
    "ff_val_black_gender0_dataset=FairFaceDataset(df_ff_val_black_gender0,'../',transform_train)\n",
    "ff_val_black_gender1_dataset=FairFaceDataset(df_ff_val_black_gender1,'../',transform_train)\n",
    "ff_val_white_gender0_dataset=FairFaceDataset(df_ff_val_white_gender0,'../',transform_train)\n",
    "ff_val_white_gender1_dataset=FairFaceDataset(df_ff_val_white_gender1,'../',transform_train)\n",
    "ff_val_middleEastern_gender0_dataset=FairFaceDataset(df_ff_val_middleEastern_gender0,'../',transform_train)\n",
    "ff_val_middleEastern_gender1_dataset=FairFaceDataset(df_ff_val_middleEastern_gender1,'../',transform_train)\n",
    "ff_val_LatinoHispanic_gender0_dataset=FairFaceDataset(df_ff_val_LatinoHispanic_gender0,'../',transform_train)\n",
    "ff_val_LatinoHispanic_gender1_dataset=FairFaceDataset(df_ff_val_LatinoHispanic_gender1,'../',transform_train)\n",
    "ff_val_southeastAsian_gender0_dataset=FairFaceDataset(df_ff_val_southeastAsian_gender0,'../',transform_train)\n",
    "ff_val_southeastAsian_gender1_dataset=FairFaceDataset(df_ff_val_southeastAsian_gender1,'../',transform_train)\n",
    "\n",
    "ff_val_eastAsian_dataloader=DataLoader(FairFaceDataset(df_ff_val_eastAsian,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_indian_dataloader=DataLoader(FairFaceDataset(df_ff_val_indian,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_black_dataloader=DataLoader(FairFaceDataset(df_ff_val_black,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_white_dataloader=DataLoader(FairFaceDataset(df_ff_val_white,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_middleEastern_dataloader=DataLoader(FairFaceDataset(df_ff_val_middleEastern,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_LatinoHispanic_dataloader=DataLoader(FairFaceDataset(df_ff_val_LatinoHispanic,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_southeastAsian_dataloader=DataLoader(FairFaceDataset(df_ff_val_eastAsian,'../',transform_train),batch_size=batch_size)\n",
    "\n",
    "ff_val_eastAsian_gender0_dataloader=DataLoader(FairFaceDataset(df_ff_val_eastAsian_gender0,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_eastAsian_gender1_dataloader=DataLoader(FairFaceDataset(df_ff_val_eastAsian_gender1,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_indian_gender0_dataloader=DataLoader(FairFaceDataset(df_ff_val_indian_gender0,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_indian_gender1_dataloader=DataLoader(FairFaceDataset(df_ff_val_indian_gender1,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_black_gender0_dataloader=DataLoader(FairFaceDataset(df_ff_val_black_gender0,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_black_gender1_dataloader=DataLoader(FairFaceDataset(df_ff_val_black_gender1,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_white_gender0_dataloader=DataLoader(FairFaceDataset(df_ff_val_white_gender0,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_white_gender1_dataloader=DataLoader(FairFaceDataset(df_ff_val_white_gender1,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_middleEastern_gender0_dataloader=DataLoader(FairFaceDataset(df_ff_val_middleEastern_gender0,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_middleEastern_gender1_dataloader=DataLoader(FairFaceDataset(df_ff_val_middleEastern_gender1,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_LatinoHispanic_gender0_dataloader=DataLoader(FairFaceDataset(df_ff_val_LatinoHispanic_gender0,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_LatinoHispanic_gender1_dataloader=DataLoader(FairFaceDataset(df_ff_val_LatinoHispanic_gender1,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_southeastAsian_gender0_dataloader=DataLoader(FairFaceDataset(df_ff_val_eastAsian_gender0,'../',transform_train),batch_size=batch_size)\n",
    "ff_val_southeastAsian_gender1_dataloader=DataLoader(FairFaceDataset(df_ff_val_eastAsian_gender1,'../',transform_train),batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 构造不平衡数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def splitByRatio(dataset,ratio):\n",
    "    return random_split(dataset,[int(len(dataset)*ratio),len(dataset)-int(len(dataset)*ratio)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unbalance_Dataset=ConcatDataset([random_split(ff_train_eastAsian_gender0_dataset,[4000,len(ff_train_eastAsian_gender0_dataset)-4000])[0],\n",
    "                                 random_split(ff_train_eastAsian_gender1_dataset,[4000,len(ff_train_eastAsian_gender1_dataset)-4000])[0],\n",
    "                                 random_split(ff_train_indian_gender0_dataset,[4000,len(ff_train_indian_gender0_dataset)-4000])[0],\n",
    "                                 random_split(ff_train_indian_gender1_dataset,[4000,len(ff_train_indian_gender1_dataset)-4000])[0],\n",
    "                                 random_split(ff_train_black_gender0_dataset,[4000,len(ff_train_black_gender0_dataset)-4000])[0],\n",
    "                                 random_split(ff_train_black_gender1_dataset,[4000,len(ff_train_black_gender1_dataset)-4000])[0],\n",
    "                                 random_split(ff_train_white_gender0_dataset,[2000,len(ff_train_white_gender0_dataset)-2000])[0],\n",
    "                                 random_split(ff_train_white_gender1_dataset,[6000,len(ff_train_white_gender1_dataset)-6000])[0],\n",
    "                                 # random_split(ff_train_middleEastern_gender0_dataset,[2000,len(ff_train_middleEastern_gender0_dataset)-2000])[0],\n",
    "                                 # random_split(ff_train_middleEastern_gender1_dataset,[2000,len(ff_train_middleEastern_gender1_dataset)-2000])[0],\n",
    "                                 # random_split(ff_train_LatinoHispanic_gender0_dataset,[4000,len(ff_train_LatinoHispanic_gender0_dataset)-4000])[0],\n",
    "                                 # random_split(ff_train_LatinoHispanic_gender1_dataset,[4000,len(ff_train_LatinoHispanic_gender1_dataset)-4000])[0],\n",
    "                                 random_split(ff_train_southeastAsian_gender0_dataset,[4000,len(ff_train_southeastAsian_gender0_dataset)-4000])[0],\n",
    "                                 random_split(ff_train_southeastAsian_gender1_dataset,[4000,len(ff_train_southeastAsian_gender1_dataset)-4000])[0]\n",
    "                                ])\n",
    "unbalance_dataloader=DataLoader(unbalance_Dataset,batch_size,shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#ratio为正确标签的比例\n",
    "def shuffle_dataset(dataset,target_index,ratio,max_range=1):\n",
    "    np.random.seed(1)\n",
    "    ds=copy.deepcopy(dataset)\n",
    "    for i,_ in enumerate(ds):\n",
    "        if np.random.rand(1)>ratio:\n",
    "            t= random.randint(0,max_range)\n",
    "            while ds[i][1][target_index] == t or t==4 or t==5:\n",
    "                t=random.randint(0,max_range)\n",
    "            ds[i][1][target_index]=t\n",
    "    return ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len_balance=len(unbalance_Dataset)/10\n",
    "balance_Dataset=ConcatDataset([\n",
    "    random_split(ff_train_eastAsian_gender0_dataset,[int(len_balance/14),len(ff_train_eastAsian_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_eastAsian_gender1_dataset,[int(len_balance/14),len(ff_train_eastAsian_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_indian_gender0_dataset,[int(len_balance/14),len(ff_train_indian_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_indian_gender1_dataset,[int(len_balance/14),len(ff_train_indian_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_black_gender0_dataset,[int(len_balance/14),len(ff_train_black_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_black_gender1_dataset,[int(len_balance/14),len(ff_train_black_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_white_gender0_dataset,[int(len_balance/14),len(ff_train_white_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_white_gender1_dataset,[int(len_balance/14),len(ff_train_white_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    # random_split(ff_train_middleEastern_gender0_dataset,[int(len_balance/14),len(ff_train_middleEastern_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    # random_split(ff_train_middleEastern_gender1_dataset,[int(len_balance/14),len(ff_train_middleEastern_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    # random_split(ff_train_LatinoHispanic_gender0_dataset,[int(len_balance/14),len(ff_train_LatinoHispanic_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    # random_split(ff_train_LatinoHispanic_gender1_dataset,[int(len_balance/14),len(ff_train_LatinoHispanic_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_southeastAsian_gender0_dataset,[int(len_balance/14),len(ff_train_southeastAsian_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_southeastAsian_gender1_dataset,[int(len_balance/14),len(ff_train_southeastAsian_gender1_dataset)-int(len_balance/14)])[0]\n",
    "                                ])\n",
    "balance_dataloader=DataLoader(balance_Dataset,batch_size=batch_size,shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len_balance=len(unbalance_Dataset)/100\n",
    "shuffle_Dataset=ConcatDataset([\n",
    "    random_split(ff_train_eastAsian_gender0_dataset,[int(len_balance/14),len(ff_train_eastAsian_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_eastAsian_gender1_dataset,[int(len_balance/14),len(ff_train_eastAsian_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_indian_gender0_dataset,[int(len_balance/14),len(ff_train_indian_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_indian_gender1_dataset,[int(len_balance/14),len(ff_train_indian_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_black_gender0_dataset,[int(len_balance/14),len(ff_train_black_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_black_gender1_dataset,[int(len_balance/14),len(ff_train_black_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_white_gender0_dataset,[int(len_balance/14),len(ff_train_white_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_white_gender1_dataset,[int(len_balance/14),len(ff_train_white_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    # random_split(ff_train_middleEastern_gender0_dataset,[int(len_balance/14),len(ff_train_middleEastern_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    # random_split(ff_train_middleEastern_gender1_dataset,[int(len_balance/14),len(ff_train_middleEastern_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    # random_split(ff_train_LatinoHispanic_gender0_dataset,[int(len_balance/14),len(ff_train_LatinoHispanic_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    # random_split(ff_train_LatinoHispanic_gender1_dataset,[int(len_balance/14),len(ff_train_LatinoHispanic_gender1_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_southeastAsian_gender0_dataset,[int(len_balance/14),len(ff_train_southeastAsian_gender0_dataset)-int(len_balance/14)])[0],\n",
    "    random_split(ff_train_southeastAsian_gender1_dataset,[int(len_balance/14),len(ff_train_southeastAsian_gender1_dataset)-int(len_balance/14)])[0]\n",
    "                                ])\n",
    "shuffle_Dataset=shuffle_dataset(shuffle_Dataset,target_index=2,ratio=-1,max_range=6)\n",
    "shuffle_dataloader=DataLoader(shuffle_Dataset,batch_size=batch_size,shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练模型的方法定义\n",
    "\n",
    "def test(loader, net,target_index):\n",
    "    net.eval()\n",
    "    correct_pred=0\n",
    "    num_examples=0\n",
    "    for batch, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target[:,target_index].to(device)\n",
    "        logits = net(data)\n",
    "        probas = F.softmax(logits,dim=1)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += target.size(0)\n",
    "        correct_pred += (predicted_labels == target).sum()\n",
    "        # acc += torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        # sum += len(target)\n",
    "        # loss_sum += loss.item()\n",
    "    print('test  acc: %.2f%% ' %(100 * correct_pred.float() / num_examples))\n",
    "    return 100 * correct_pred.float()/ num_examples\n",
    "\n",
    "def train(loader, model, target_index, training_type):\n",
    "    '''\n",
    "    :param loader:\n",
    "    :param model:\n",
    "    :param target_index: 标签下标\n",
    "    :param training_type: 模型名称\n",
    "    :return:\n",
    "    '''\n",
    "    model.train()\n",
    "    sum = 0.0\n",
    "    correct_pred=0.0\n",
    "    for batch, (data, target) in tqdm.tqdm( enumerate(loader),desc=\"模型训练中：\", total=len(loader)):\n",
    "        data, target = data.to(device), target[:,target_index].type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        probas = F.softmax(logits,dim=1)\n",
    "        cost = F.cross_entropy(logits, target)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(probas, 1)\n",
    "        sum += target.size(0)\n",
    "        correct_pred += (predicted == target).sum()\n",
    "    acc=100 * correct_pred.float() / sum\n",
    "    print('train acc: %.2f%%' % (acc))\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, \"../models/23_1_4/\" + str(training_type) + \"_checkpoint.pth\")\n",
    "    if correct_pred==sum:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def testRES(loader, net,target_index):\n",
    "    net.eval()\n",
    "    correct_pred=0\n",
    "    num_examples=0\n",
    "    for batch, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target[:,target_index].to(device)\n",
    "        probas = net(data)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += target.size(0)\n",
    "        correct_pred += (predicted_labels == target).sum()\n",
    "        # acc += torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        # sum += len(target)\n",
    "        # loss_sum += loss.item()\n",
    "    print('test  acc: %.2f%% ' %(100 * correct_pred.float() / num_examples))\n",
    "    return 100 * correct_pred.float()/ num_examples\n",
    "\n",
    "def FairFaceTest(model,target_index):\n",
    "    print(\"全部测试集：\")\n",
    "    testRES(ff_val_dataloader,model,target_index=target_index)\n",
    "    n=[]\n",
    "    print(\"eastAsian gender0测试集：\")\n",
    "    n.append(testRES(ff_val_eastAsian_gender0_dataloader,model,target_index=target_index).item())\n",
    "    print(\"eastAsian gender1测试集：\")\n",
    "    n.append(testRES(ff_val_eastAsian_gender1_dataloader,model,target_index=target_index).item())\n",
    "    print(\"indian gender0测试集：\")\n",
    "    n.append(testRES(ff_val_indian_gender0_dataloader,model,target_index=target_index).item())\n",
    "    print(\"indian gender1测试集：\")\n",
    "    n.append(testRES(ff_val_indian_gender1_dataloader,model,target_index=target_index).item())\n",
    "    print(\"black gender0测试集：\")\n",
    "    n.append(testRES(ff_val_black_gender0_dataloader,model,target_index=target_index).item())\n",
    "    print(\"black gender1测试集：\")\n",
    "    n.append(testRES(ff_val_black_gender1_dataloader,model,target_index=target_index).item())\n",
    "    print(\"white gender0测试集：\")\n",
    "    n.append(testRES(ff_val_white_gender0_dataloader,model,target_index=target_index).item())\n",
    "    print(\"white gender1测试集：\")\n",
    "    n.append(testRES(ff_val_white_gender1_dataloader,model,target_index=target_index).item())\n",
    "    # print(\"middleEastern gender0测试集：\")\n",
    "    # n.append(testRES(ff_val_middleEastern_gender0_dataloader,model,target_index=target_index).item())\n",
    "    # print(\"middleEastern gender1测试集：\")\n",
    "    # n.append(testRES(ff_val_middleEastern_gender1_dataloader,model,target_index=target_index).item())\n",
    "    # print(\"LatinoHispanic gender0测试集：\")\n",
    "    # n.append(testRES(ff_val_LatinoHispanic_gender0_dataloader,model,target_index=target_index).item())\n",
    "    # print(\"LatinoHispanic gender1测试集：\")\n",
    "    # n.append(testRES(ff_val_LatinoHispanic_gender1_dataloader,model,target_index=target_index).item())\n",
    "    print(\"southeastAsian gender0测试集：\")\n",
    "    n.append(testRES(ff_val_southeastAsian_gender0_dataloader,model,target_index=target_index).item())\n",
    "    print(\"southeastAsian gender1测试集：\")\n",
    "    n.append(testRES(ff_val_southeastAsian_gender1_dataloader,model,target_index=target_index).item())\n",
    "    print(\"方差：\")\n",
    "    print(np.var(n))\n",
    "    return n\n",
    "def load_model(model_path=None):\n",
    "    net = torchvision.models.resnet34()\n",
    "    global optimizer\n",
    "    optimizer= torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    net = net.to(device)\n",
    "    if model_path!=None:\n",
    "        checkpoint = torch.load(model_path)\n",
    "        net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # if torch.cuda.device_count() > 1:\n",
    "    #     print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    #     net= nn.DataParallel(net)\n",
    "    return net"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "#原始训练\n",
    "net=load_model()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "for epoch in range(50):\n",
    "        print('epoch: %d' % epoch)\n",
    "        train(unbalance_dataloader,net,target_index=2,training_type=\"fairface_RESNET_race\")#后两个标签_敏感特征\n",
    "        if (epoch+1)%5==0:\n",
    "            FairFaceTest(net,target_index=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#混淆训练\n",
    "net=load_model('../models/22_12_30/fairface_RESNET_race_checkpoint.pth')\n",
    "# optimizer.param_groups[0][\"lr\"]=0.001\n",
    "for epoch in range(1):\n",
    "        print('epoch: %d' % epoch)\n",
    "        train(shuffle_dataloader,net,target_index=2,training_type=\"fairface_shuffle_RESNET_race\")\n",
    "n=FairFaceTest(net,target_index=2)\n",
    "for i in n:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "#恢复训练\n",
    "net1=load_model('../models/23_1_4/fairface_shuffle_RESNET_race_checkpoint.pth')\n",
    "for epoch in range(1000):\n",
    "        print('epoch: %d' % epoch)\n",
    "        if train(balance_dataloader,net1,target_index=2,training_type=\"fairface_balance_RESNET_race\") :\n",
    "            break\n",
    "n=FairFaceTest(net1,target_index=2)\n",
    "for i in n:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net2=load_model('../models/22_12_30/fairface_RESNET_race_checkpoint.pth')\n",
    "for epoch in range(1000):\n",
    "        print('epoch: %d' % epoch)\n",
    "        if train(balance_dataloader,net2,target_index=2,training_type=\"fairface_finetune_RESNET_race\") :\n",
    "            break\n",
    "n=FairFaceTest(net2,target_index=2)\n",
    "for i in n:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net3=load_model('../models/23_1_3/fairface_RESNET_race_checkpoint.pth')\n",
    "params_1x = [param for name, param in net3.named_parameters()\n",
    "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "optimizer= torch.optim.Adam([{'params': params_1x},\n",
    "                               {'params': net3.fc.parameters(),\n",
    "                                'lr': 0.001 }],lr=0.0003)\n",
    "for epoch in range(100):\n",
    "        print('epoch: %d' % epoch)\n",
    "        if train(balance_dataloader,net3,target_index=2,training_type=\"fairface_finetune_RESNET_race\") :\n",
    "            break\n",
    "n=FairFaceTest(net3,target_index=2)\n",
    "for i in n:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
